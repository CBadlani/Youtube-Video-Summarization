{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEZ5ZpZTK6oY",
        "outputId": "f27bf6e0-b66e-4c45-e319-abefff6a8912"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/57.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "! pip install pytube -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xcbue3SNz1K",
        "outputId": "df4393bc-2e55-44a1-bff2-69489c2a893a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kenlm\n",
            "  Downloading kenlm-0.2.0.tar.gz (427 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m427.4/427.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: kenlm\n",
            "  Building wheel for kenlm (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kenlm: filename=kenlm-0.2.0-cp310-cp310-linux_x86_64.whl size=3184434 sha256=897ce76d67baa8acecdbc4b28bbef612bb926cb1aa5df7c7de28f699b9349f13\n",
            "  Stored in directory: /root/.cache/pip/wheels/fd/80/e0/18f4148e863fb137bd87e21ee2bf423b81b3ed6989dab95135\n",
            "Successfully built kenlm\n",
            "Installing collected packages: kenlm\n",
            "Successfully installed kenlm-0.2.0\n",
            "Collecting pyctcdecode\n",
            "  Downloading pyctcdecode-0.5.0-py2.py3-none-any.whl (39 kB)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from pyctcdecode) (1.25.2)\n",
            "Collecting pygtrie<3.0,>=2.1 (from pyctcdecode)\n",
            "  Downloading pygtrie-2.5.0-py3-none-any.whl (25 kB)\n",
            "Collecting hypothesis<7,>=6.14 (from pyctcdecode)\n",
            "  Downloading hypothesis-6.103.0-py3-none-any.whl (459 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.9/459.9 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from hypothesis<7,>=6.14->pyctcdecode) (23.2.0)\n",
            "Requirement already satisfied: sortedcontainers<3.0.0,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from hypothesis<7,>=6.14->pyctcdecode) (2.4.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from hypothesis<7,>=6.14->pyctcdecode) (1.2.1)\n",
            "Installing collected packages: pygtrie, hypothesis, pyctcdecode\n",
            "Successfully installed hypothesis-6.103.0 pyctcdecode-0.5.0 pygtrie-2.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install kenlm\n",
        "!pip install pyctcdecode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "onLk2E0qLIMm",
        "outputId": "6c5d2473-a98d-44a0-aae9-d774c674ae08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the YouTube video URL: https://youtu.be/ZU0f8_C5Pm0?si=CWDTzIQA4suFxvT-\n",
            "Download completed!\n"
          ]
        }
      ],
      "source": [
        "from pytube import YouTube\n",
        "\n",
        "# YouTube video ka URL\n",
        "video_url = input(\"Enter the YouTube video URL: \")\n",
        "\n",
        "# Create a YouTube object\n",
        "yt = YouTube(video_url)\n",
        "\n",
        "# Get the highest resolution stream available\n",
        "video_stream = yt.streams.get_highest_resolution()\n",
        "\n",
        "# Download the video\n",
        "video_stream.download('D:\\MP')\n",
        "\n",
        "print(\"Download completed!\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "go76rsAQ5mh9",
        "outputId": "e3b3e1a4-4fcd-4377-809e-07bf824ab0a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Video Title: ONE SHOT - 1 Minute Motivational Video\n",
            "Video Description: Daily Motivation:\n",
            "Facebook : @Michel.David.inspiration\n",
            "Twitter : @_Michel_David_\n",
            "Instagram : @Michel.David.inspiration  \n",
            "\n",
            "Fair Use Disclaimer:\n",
            "Our purpose is to produce quality inspirational, educational and motivational video content, and to share it with our viewers.\n",
            "\n",
            "This video has no negative impact on the original works.\n",
            "This video is used for educational purposes.\n",
            "This video is transformative in nature.\n",
            "This channel’s owner claims no copyright, and cannot be held accountable.\n",
            "\n",
            "If you are the legal content owner of any videos used here and would like them removed, please contact us.  Any infringement was not done on purpose and will be rectified to the satisfaction of all parties.\n",
            "Video Duration (seconds): 59\n",
            "Video Views: 323377\n",
            "Audio download completed!\n"
          ]
        }
      ],
      "source": [
        "# Print video details\n",
        "print(\"Video Title:\", yt.title)\n",
        "print(\"Video Description:\", yt.description)\n",
        "print(\"Video Duration (seconds):\", yt.length)\n",
        "print(\"Video Views:\", yt.views)\n",
        "audio_stream = yt.streams.filter(only_audio=True, file_extension='mp4').first()\n",
        "\n",
        "# Download the audio to the specified path and filename\n",
        "audio_stream.download(output_path='D:/MP', filename='ytaudio3.mp4')\n",
        "\n",
        "print(\"Audio download completed!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gEFWwRw6odD",
        "outputId": "6668f472-bfba-41d7-b207-29877c927ebc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at facebook/wav2vec2-base-960h were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
            "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': \"THE BIGEST POISON IN US IS REGRET BUT THINK THERE'RE A LOT OF PEOPLE THAT HAVE DREAMS AND ASPIRATIONS OF THINGS THAT THEY ALWAYS WANTED TO DO BUT IT WASN'T THE RIGHT TIME BOR THEY DIDN'T HAVE ENOUGH MONEY OR THEYD HAVE ENOUGH EXPERIENCE IT'S NEVER THE RIGHT TIME YOU NEVER CAN HAVE THE RIGHT EXPERIENCE AND ALL THE SUN YOU WAKE UP AND YOU'RE SEVENTY AND YOU' LIKE GOT I WISH I COULD HA TONE IT'S WHAT YOU DON'T DO THAT SCREAMS WITH YOU LATER ON I GET ONE CHOT AT THIS LIKE THIS IS IT AND I DON'T WANT TO GO TO LIKE THING TO EIGHT PER CENT PERSNIN I DON'TW LO BACK I'M TY LIKE SEVENTY SEVENTY LIKE I DIDN'T DO THAT THERE'S SO MUCH I WANT TO DO AND I LOV LIKE SO MUCH I DON'T WANT TO MISSIT\"}\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "\n",
        "command = 'ffmpeg -i file:D:/MP/ytaudio.mp4 -acodec pcm_s16le -ar 16000 D:/MP/ytaudio3.wav'\n",
        "subprocess.run(command, shell=True)\n",
        "\n",
        "!pip install transformers -q\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load the speech recognition model from Hugging Face\n",
        "speech_recognition = pipeline(task=\"automatic-speech-recognition\", model=\"facebook/wav2vec2-base-960h\")\n",
        "\n",
        "# Replace 'D:/MP/ytaudio.mp4' with the correct path to your audio file\n",
        "audio_file_path = 'D:/MP/ytaudio3.mp4'\n",
        "\n",
        "# Perform speech recognition on the audio file\n",
        "transcription = speech_recognition(audio_file_path)\n",
        "\n",
        "# Print the transcribed text\n",
        "print(transcription)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173,
          "referenced_widgets": [
            "6d91ddedea3f40b7af420e17b82be977",
            "9a299852c437457eb1da3633680bd157",
            "17830ff1abf84637be9a1bf6b8b7992c",
            "d390a8591af044548fceea5d9f63c955",
            "b5cae4b8864740318fa03d6091bc6ce8",
            "0061adf1c12941c98d4e0b034c2e63c9",
            "67ab179ace654a9c96b3d18651b3d431",
            "14c0d8a3118c43af80be5ffb56c047d5",
            "191fe93100e346af83da255dead4e7fc",
            "4a58e314f4fe4bf199ec9c2ca700b408",
            "df76cfb157484a9bbf1a2c54928220e3"
          ]
        },
        "id": "cMdF8tw76zLq",
        "outputId": "745bedff-448f-483d-b7a3-ee29d3134323"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at jonatasgrosman/wav2vec2-large-xlsr-53-english were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
            "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at jonatasgrosman/wav2vec2-large-xlsr-53-english and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6d91ddedea3f40b7af420e17b82be977"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': \"the biggest poison in us is regret i think there are a lot of people have dreams and aspirations of things that he always wanted to do but it wasn't the right time the n't have enough money or end have of experience it's never the right time you never can have the right experience and sun you wake up an yoseventhyad you i wish i could have done it it's what you don't dooms with you later on i got one shot at this lifei it and i don't want to go to living to eight percent version of my i don't look back and like seventy-seven i didn't do that so much i want to do and i love life so much i don't want to miss it\"}\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load the speech recognition model from Hugging Face\n",
        "speech_recognition = pipeline(task=\"automatic-speech-recognition\", model=\"jonatasgrosman/wav2vec2-large-xlsr-53-english\", device=device)\n",
        "\n",
        "# Replace 'D:/MP/ytaudio.mp4' with the correct path to your audio file\n",
        "audio_file_path = 'D:/MP/ytaudio3.mp4'\n",
        "\n",
        "# Perform speech recognition on the audio file\n",
        "transcription = speech_recognition(audio_file_path)\n",
        "\n",
        "# Print the transcribed text\n",
        "print(transcription)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfM2mdoF7Mhw",
        "outputId": "833cc3b6-bf0d-4dd5-fe5d-c8e4567eb88f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Your max_length is set to 150, but your input_length is only 149. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=74)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:\n",
            "the biggest poison in us is regret i think there are a lot of people have dreams and aspirations of things that he always wanted to do but it wasn't the right time the n't have enough money or end have of experience it's never the right time you never can have the right experience and sun you wake up an yoseventhyad you i wish i could have done it it's what you don't dooms with you later on i got one shot at this lifei it and i don't want to go to living to eight percent version of my i don't look back and like seventy-seven i didn't do that so much i want to do and i love life so much i don't want to miss it\n",
            "\n",
            "Summarized Text:\n",
            " The biggest poison in us is regret, says the author . \"What you don't dooms with you later on later on i got one shot at this lifei it and i don't want to go to living to eight percent version of my\"\n"
          ]
        }
      ],
      "source": [
        "# Get the transcribed text\n",
        "transcribed_text = transcription['text']\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load the summarization model from Hugging Face\n",
        "summarizer = pipeline(\"summarization\")\n",
        "\n",
        "\n",
        "# Perform summarization\n",
        "\n",
        "summary = summarizer(transcribed_text, max_length=150, min_length=50, length_penalty=2.0, num_beams=4)\n",
        "\n",
        "\n",
        "# Print the summarized text\n",
        "print(\"Original Text:\")\n",
        "print(transcribed_text)\n",
        "print(\"\\nSummarized Text:\")\n",
        "print(summary[0]['summary_text'])\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6d91ddedea3f40b7af420e17b82be977": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9a299852c437457eb1da3633680bd157",
              "IPY_MODEL_17830ff1abf84637be9a1bf6b8b7992c",
              "IPY_MODEL_d390a8591af044548fceea5d9f63c955"
            ],
            "layout": "IPY_MODEL_b5cae4b8864740318fa03d6091bc6ce8"
          }
        },
        "9a299852c437457eb1da3633680bd157": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0061adf1c12941c98d4e0b034c2e63c9",
            "placeholder": "​",
            "style": "IPY_MODEL_67ab179ace654a9c96b3d18651b3d431",
            "value": "Fetching 4 files: 100%"
          }
        },
        "17830ff1abf84637be9a1bf6b8b7992c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_14c0d8a3118c43af80be5ffb56c047d5",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_191fe93100e346af83da255dead4e7fc",
            "value": 4
          }
        },
        "d390a8591af044548fceea5d9f63c955": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a58e314f4fe4bf199ec9c2ca700b408",
            "placeholder": "​",
            "style": "IPY_MODEL_df76cfb157484a9bbf1a2c54928220e3",
            "value": " 4/4 [00:00&lt;00:00, 186.95it/s]"
          }
        },
        "b5cae4b8864740318fa03d6091bc6ce8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0061adf1c12941c98d4e0b034c2e63c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "67ab179ace654a9c96b3d18651b3d431": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "14c0d8a3118c43af80be5ffb56c047d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "191fe93100e346af83da255dead4e7fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4a58e314f4fe4bf199ec9c2ca700b408": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df76cfb157484a9bbf1a2c54928220e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}